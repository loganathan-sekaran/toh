# Quick Reference: New Training System

## ğŸ¯ What Changed?

**Problem**: Agent couldn't identify which early action caused later penalties

**Solution**: Temporal Credit Assignment + Prioritized Experience Replay

---

## ğŸ“Š Expected Improvements

| Metric | Before | After (Expected) |
|--------|--------|------------------|
| Episodes to 90% (3 discs) | 800-1000 | 500-700 |
| Episodes to 70% (4 discs) | 1500-2000 | 1000-1500 |
| Oscillation loops | Common | Rare after ep 100 |
| Learning speed | Gradual | Faster |

---

## ğŸ”§ How It Works (Simple)

### When agent gets big penalty (like -20):

**Old System**:
```
âŒ Only current action gets penalty
âŒ Agent forgets what led to this
```

**New System**:
```
âœ… Current action gets penalty Ã— 3 = -60
âœ… Previous action gets extra -30 penalty
âœ… 2 steps back gets extra -15 penalty
âœ… 3 steps back gets extra -7.5 penalty
âœ… Agent learns the whole sequence was bad
```

### During training:

**Old System**:
```
âŒ Random sampling â†’ mostly learning from successes
```

**New System**:
```
âœ… 50% of batch = penalty experiences (mistakes)
âœ… 30% of batch = neutral experiences
âœ… 20% of batch = reward experiences (successes)
âœ… Focused learning from mistakes!
```

---

## ğŸš€ How to Use

### 1. Train New Model

```bash
./start_gui.sh
# Select "Train Model"
# Choose:
#   - 3 or 4 discs
#   - Large architecture (128-64-32)
#   - 1000-1500 episodes
```

### 2. Watch for These Messages

```
Step 42:
  Reward: -25.0
  ğŸ”— Credit assignment: Propagated -37.5 penalty to previous action
```

This means it's working! âœ¨

### 3. Test with Exploration

```
Test Configuration:
  - Exploration: 15% (recommended)
  - Click "Test Again" if it fails first time
```

---

## ğŸ“ˆ Training Tips

### For 3 Discs:
- **Architecture**: Large (128-64-32)
- **Episodes**: 1000
- **Expected**: 90%+ success, 8-12 avg steps

### For 4 Discs:
- **Architecture**: Extra Large (256-128-64)
- **Episodes**: 1500-2000
- **Expected**: 70-85% success, 18-30 avg steps

### For Multiple Discs (3+4):
- **Architecture**: Extra Large (256-128-64)
- **Episodes**: 2500
- **Train on 3 first, then 4**: Better generalization

---

## ğŸ” Debugging

### If model still gets stuck:

1. **Check success rate**:
   - <50%: Needs more training
   - 50-80%: Use exploration 20% during testing
   - >80%: Should work well

2. **Look at avg steps**:
   - Close to optimal (7 for 3 discs, 15 for 4): Excellent
   - 2-3Ã— optimal: Good
   - >5Ã— optimal: Needs more training

3. **During testing**:
   - Use exploration 15-20%
   - Retest multiple times
   - Each run tries different path

### Messages to Watch For:

âœ… **Good Signs**:
```
ğŸ”— Credit assignment: Propagated -X penalty
âš ï¸ Suboptimal move (penalty: -5.0)  [Not true invalid]
Episode 500: Success=Yes, Steps=8
```

âŒ **Bad Signs**:
```
âš ï¸ TRULY INVALID MOVE (rule violation)
âš ï¸ STUCK IN LOOP
Episode 500: Success=No, Steps=75
```

---

## âš™ï¸ Advanced Tuning

If you want to experiment, edit `dqn_agent.py`:

```python
# Stronger credit assignment (lines 33-35)
self.trajectory_penalty_propagation = 0.7  # Default: 0.5
self.penalty_scale = 3.0                   # Default: 2.0
self.oscillation_penalty_scale = 4.0       # Default: 3.0

# Deeper propagation (line 127)
propagation_depth = 8  # Default: 5

# More focus on penalties (line 249)
n_penalties = int(self.batch_size * 0.6)  # Default: 0.5 (50%)
```

---

## ğŸ“š More Info

- **`BACKTRACKING_SOLUTION.md`** - Why this works
- **`CREDIT_ASSIGNMENT_VISUAL.md`** - Visual diagrams
- **`BACKTRACKING_CHANGES_SUMMARY.md`** - Detailed changes

---

## ğŸ“ Key Concepts

### Temporal Credit Assignment
> "When you get a reward/penalty, credit/blame the actions that led to it"

### Prioritized Experience Replay  
> "Learn more from mistakes, but don't forget successes"

### Exploration During Testing
> "Try different paths to find solutions"

---

## â“ FAQ

**Q: Do I need to retrain existing models?**
A: No, but new models will train better

**Q: Will this fix poorly trained models?**
A: No - use exploration during testing instead

**Q: Is this true backtracking?**
A: No - it's credit assignment. Agent learns causal relationships.

**Q: Should I use higher exploration?**
A: 10-20% is good balance. Higher = more random, slower solving.

**Q: Why still failing sometimes?**
A: DQN is probabilistic. Not guaranteed 100% success. Retest helps.

---

## ğŸ¯ Bottom Line

**Before**: Agent learned slowly, got stuck often

**After**: Agent learns faster, understands action consequences better

**Your models will be smarter! ğŸ§ âœ¨**
